{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emanuele.dallalonga/miniconda3/envs/musictagger/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, zero_one_loss, auc\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import librosa\n",
    "import pickle as pk\n",
    "\n",
    "from keras.layers import Conv2D, MaxPool2D, Activation, Dense, Input, Flatten, BatchNormalization, Dropout\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import Sequence\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hardware Parameters\n",
    "n_gpus = 4\n",
    "\n",
    "#Mel parameters\n",
    "sr = 22050\n",
    "n_sample_fft = 2048 \n",
    "hop_length = 512\n",
    "\n",
    "#Training Parameters\n",
    "batch_size = 32\n",
    "max_epochs = 200\n",
    "max_trainings = 5\n",
    "kernel_initializer = 'glorot_uniform'#'he_uniform'\n",
    "\n",
    "if batch_size % n_gpus != 0:\n",
    "    print(\"Batch size should be dividibile per n_gpus\")\n",
    "\n",
    "# SGD parameters\n",
    "starting_learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "global_decay = 0.2\n",
    "local_decay = 1e-6\n",
    "\n",
    "# EarlyStopping Parameters\n",
    "min_improvement = 0\n",
    "patience = 10\n",
    "\n",
    "# Paths\n",
    "dataset_dir = '../data/MagnaTagATune/MEL_default_hop/'\n",
    "annotations_path = '../data/MagnaTagATune/annotation_reduced_50.csv'\n",
    "\n",
    "checkpoint_dir = './checkpoints_MEL_32f/'\n",
    "checkpoint_file_name = 'weights-{epoch:03d}-{val_loss:.5f}.hdf5'\n",
    "log_dir ='./logs_MEL_32f/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Align dataset split to batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_split(split, batch_size, num_songs):\n",
    "    num_songs_split = split*num_songs\n",
    "    return int(num_songs_split - num_songs_split%batch_size)/num_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data reading during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MagnaTagATuneSequence(Sequence):\n",
    "\n",
    "    def __init__(self, train_set_paths, train_set_labels, batch_size):\n",
    "        self.paths, self.y = train_set_paths, train_set_labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_paths = self.paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x = []\n",
    "        for value in batch_x_paths:\n",
    "            path = dataset_dir + value[:-3]+'p'\n",
    "            S = pk.load(open(path,'rb'))\n",
    "            batch_x.append(S)\n",
    "        batch_x = np.array(batch_x)[:,:,:,np.newaxis]        \n",
    "        return (batch_x,batch_y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Performance Metrics (not used anymore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_wrong_over_correct_ones(y_true, y_pred):\n",
    "    op1 = K.sum(K.abs(K.cast(y_true - K.round(y_pred), dtype='float32')))\n",
    "    op2 = K.sum(K.cast(K.equal(y_true,1.0),dtype='float32'))\n",
    "    return op1/op2\n",
    "\n",
    "def ratio_correct_ones(y_true, y_pred):\n",
    "    op1 = K.sum(K.cast(K.equal(y_true + K.round(y_pred),2.0),dtype='float32'))\n",
    "    op2 = K.sum(K.cast(K.equal(y_true,1.0),dtype='float32'))\n",
    "    return op1/op2\n",
    "\n",
    "def auc_roc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.metrics.auc(y_true, y_pred, summation_method='careful_interpolation')\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Best checkpoint selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_checkpoint(prev_chkpts):\n",
    "    best_ratio = np.inf\n",
    "    best_chkpt = ''\n",
    "    best_epoch = 0\n",
    "    for chkpt in prev_chkpts:\n",
    "        epoch = int(chkpt[8:11])\n",
    "        ratio = float(chkpt[12:19])\n",
    "        \n",
    "        if ratio < best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_chkpt = chkpt\n",
    "            best_epoch = epoch\n",
    "    print('\\n starting from model {} \\n'.format(best_chkpt))\n",
    "    return best_chkpt, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 118/1920 [00:00<00:01, 1175.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Train set size: 22400\n",
      "Test set size: 3460 \n",
      "\n",
      "Train set size: 20480\n",
      "Validation set size: 1920 \n",
      "\n",
      "X dimension: (128, 911)\n",
      "Y dimension: 50 \n",
      "\n",
      "\n",
      "* * * Loading Validation Set into Memory * * *\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1920/1920 [00:01<00:00, 1181.19it/s]\n"
     ]
    }
   ],
   "source": [
    "annotations = pd.read_csv(annotations_path, sep='\\t')\n",
    "\n",
    "tot_t_size = 0.866203\n",
    "tot_train_set, test_set = train_test_split(annotations, train_size=tot_t_size, test_size=(1-tot_t_size), random_state=42) \n",
    "\n",
    "print(\"Complete Train set size: {}\".format(tot_train_set.shape[0]))\n",
    "print(\"Test set size: {} \\n\".format(test_set.shape[0]))\n",
    "\n",
    "t_size = 0.91429\n",
    "train_set, val_set = train_test_split(tot_train_set, train_size=t_size, test_size=(1-t_size), random_state=42) \n",
    "\n",
    "print(\"Train set size: {}\".format(train_set.shape[0]))\n",
    "print(\"Validation set size: {} \\n\".format(val_set.shape[0]))\n",
    "\n",
    "train_set_paths = train_set['mp3_path'].values\n",
    "train_set_labels = train_set.drop(columns=['mp3_path','Unnamed: 0']).values\n",
    "\n",
    "y_dimension = train_set_labels.shape[1]\n",
    "S = pk.load(open(dataset_dir + annotations['mp3_path'][0][:-3]+ 'p','rb'))\n",
    "x_dimension = S.shape\n",
    "\n",
    "print(\"X dimension: {}\\nY dimension: {} \\n\".format(x_dimension, y_dimension))\n",
    "\n",
    "   \n",
    "val_set_paths = val_set['mp3_path'].values\n",
    "val_set_labels = val_set.drop(columns=['mp3_path','Unnamed: 0']).values\n",
    "    \n",
    "\n",
    "    \n",
    "print('\\n* * * Loading Validation Set into Memory * * *\\n')\n",
    "\n",
    "val_set_data = []\n",
    "for value in tqdm(val_set_paths):\n",
    "    path = dataset_dir+value[:-3]+'p'\n",
    "    S = pk.load(open(path,'rb'))\n",
    "    val_set_data.append(S)  \n",
    "val_set_data = np.array(val_set_data)[:,:,:,np.newaxis] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot train set shape(df): (20480, 52)\n",
      "Tot train set paths: (20480,)\n",
      "Song from train set (df): 9/domased-slowdown-07-too_slow-117-146.mp3\n",
      "Song from train set paths: 9/domased-slowdown-07-too_slow-117-146.mp3\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#pick up random song in training\n",
    "#np.random.seed(0)\n",
    "random_song = np.random.randint(0,train_set.shape[0],)\n",
    "song_path = train_set.iloc[random_song]['mp3_path']\n",
    "print('Tot train set shape(df): {}'.format(train_set.shape))\n",
    "print('Tot train set paths: {}'.format(train_set_paths.shape))\n",
    "print('Song from train set (df): {}'.format(train_set.iloc[random_song]['mp3_path']))\n",
    "print('Song from train set paths: {}'.format(train_set_paths[random_song]))\n",
    "\n",
    "labels_from_annotation = annotations.loc[annotations['mp3_path'] == song_path]\n",
    "print(labels_from_annotation.values[0][1:-1])\n",
    "\n",
    "print(train_set_labels[random_song])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Modify session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 32\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=n_filters, kernel_size=(3,3),padding='valid',data_format='channels_last', input_shape=(x_dimension[0],x_dimension[1],1)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=n_filters*2, kernel_size=(3,3),padding='valid',data_format='channels_last', input_shape=(x_dimension[0],x_dimension[1],1)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=n_filters*2, kernel_size=(3,3),padding='valid',data_format='channels_last', input_shape=(x_dimension[0],x_dimension[1],1)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=n_filters*2, kernel_size=(3,3),padding='valid',data_format='channels_last', input_shape=(x_dimension[0],x_dimension[1],1)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=y_dimension, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 126, 909, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 126, 909, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 126, 909, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 63, 454, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 63, 454, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 61, 452, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 61, 452, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 61, 452, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30, 226, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 226, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 224, 64)       36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 224, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 224, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 112, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 14, 112, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 110, 64)       36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 110, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12, 110, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 55, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 55, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 21120)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                1056050   \n",
      "=================================================================\n",
      "Total params: 1,149,618\n",
      "Trainable params: 1,149,170\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Callbacks definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallBack(keras.callbacks.Callback):\n",
    "    def __init__(self, callbacks, model, is_tb=False):\n",
    "            super().__init__()\n",
    "            self.callback = callbacks\n",
    "            self.is_tb = is_tb\n",
    "            if not self.is_tb:\n",
    "                self.model = model\n",
    "                self.model_original = model\n",
    "\n",
    "    def on_epoch_begin(self,epoch,logs=None):\n",
    "            if not self.is_tb:\n",
    "                self.model = self.model_original\n",
    "            self.callback.on_epoch_begin(epoch, logs=logs)\n",
    "\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "            if not self.is_tb:\n",
    "                self.model = self.model_original\n",
    "            else:\n",
    "                y_pred = self.model.predict(self.validation_data[0])\n",
    "                auc_skl = roc_auc_score(self.validation_data[1], y_pred)\n",
    "                print('\\nSKLearn validation auc: {}'.format(auc_skl))\n",
    "            self.callback.on_epoch_end(epoch, logs=logs)\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "            if not self.is_tb:\n",
    "                self.model = self.model_original\n",
    "            self.callback.on_batch_end(batch, logs=logs)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "            if not self.is_tb:\n",
    "                self.model = self.model_original\n",
    "            self.callback.on_batch_begin(batch, logs=logs)\n",
    "            \n",
    "    def on_train_begin(self, logs=None):\n",
    "            if not self.is_tb:\n",
    "                self.model = self.model_original\n",
    "            self.callback.set_model(self.model)\n",
    "            self.callback.on_train_begin(logs=logs)\n",
    "\n",
    "\n",
    "cbk_tb = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, batch_size=batch_size, write_graph=True,\n",
    "                                         write_grads=False, write_images=False, embeddings_freq=0,\n",
    "                                         embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "cbk_es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n",
    "                                          min_delta=min_improvement, patience=patience, verbose=1)\n",
    "\n",
    "cbk_mc = keras.callbacks.ModelCheckpoint(monitor='val_loss', mode='min', save_best_only=True, \n",
    "                                            filepath=checkpoint_dir+checkpoint_file_name, \n",
    "                                            verbose=1)\n",
    "\n",
    "cbk = MyCallBack(cbk_tb, model, is_tb=True)\n",
    "cbk2 = MyCallBack(cbk_mc, model)\n",
    "\n",
    "callbacks = [cbk,cbk_es,cbk2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "* * * * Starting training 0 from epoch 1 * * * * \n",
      "\n",
      "\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Epoch 1/200\n",
      "640/640 [==============================] - 92s 144ms/step - loss: 0.1689 - val_loss: 0.1658\n",
      "\n",
      "SKLearn validation auc: 0.8180300086236307\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16576, saving model to ./checkpoints_MEL_32f/weights-001-0.16576.hdf5\n",
      "Epoch 2/200\n",
      " 89/640 [===>..........................] - ETA: 1:15 - loss: 0.1519"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2baa99a38c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     parallel_model.fit_generator(MagnaTagATuneSequence(train_set_paths, train_set_labels, batch_size),\n\u001b[1;32m     39\u001b[0m                                  \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_set_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                                  epochs=max_epochs, callbacks = callbacks, initial_epoch = initial_epoch)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/musictagger/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/musictagger/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/musictagger/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/musictagger/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/musictagger/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/musictagger/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/musictagger/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/musictagger/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/musictagger/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/musictagger/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_epoch = 0\n",
    "training_nr = 0\n",
    "\n",
    "parallel_model = keras.utils.multi_gpu_model(model, gpus=n_gpus)\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "\n",
    "while (initial_epoch <= max_epochs) and (training_nr <= max_trainings):\n",
    "    \n",
    "    best_checkpoint = ''\n",
    "    best_epoch = 0\n",
    "    \n",
    "    previous_checkpoints = os.listdir(checkpoint_dir)\n",
    "    \n",
    "    if previous_checkpoints != []:\n",
    "        best_checkpoint, best_epoch = find_best_checkpoint(previous_checkpoints)\n",
    "        initial_epoch = best_epoch             \n",
    "    \n",
    "    print('\\n\\n* * * * Starting training {0} from epoch {1} * * * * \\n\\n'.format(training_nr,  initial_epoch+1))\n",
    "    \n",
    "    #update lr\n",
    "    decay = global_decay ** training_nr\n",
    "    learning_rate = starting_learning_rate * decay\n",
    "    \n",
    "    \n",
    "    training_nr = training_nr + 1\n",
    "    \n",
    "    optimizer = SGD(lr=learning_rate, momentum=momentum, decay=local_decay , nesterov=True)\n",
    "    \n",
    "    if len(previous_checkpoints)!=0:\n",
    "        model.load_weights(checkpoint_dir + best_checkpoint)\n",
    "        parallel_model = keras.utils.multi_gpu_model(model, gpus=n_gpus)\n",
    "    \n",
    "    \n",
    "    parallel_model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "    \n",
    "    parallel_model.fit_generator(MagnaTagATuneSequence(train_set_paths, train_set_labels, batch_size),\n",
    "                                 validation_data = (val_set_data, val_set_labels),\n",
    "                                 epochs=max_epochs, callbacks = callbacks, initial_epoch = initial_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_paths = test_set['mp3_path'].values\n",
    "test_set_labels = test_set.drop(columns=['mp3_path','Unnamed: 0']).values\n",
    "test_set_size = len(test_set_paths)\n",
    "print(\"Test set size: {} \".format(test_set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_checkpoints = os.listdir(checkpoint_dir)\n",
    "best_checkpoint, best_epoch = find_best_checkpoint(previous_checkpoints)\n",
    "#model.load_weights(checkpoint_dir + best_checkpoint)\n",
    "model = keras.models.load_model(checkpoint_dir + best_checkpoint)\n",
    "parallel_model = keras.utils.multi_gpu_model(model, gpus=n_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = parallel_model.predict_generator(MagnaTagATuneSequence(test_set_paths, test_set_labels, batch_size), verbose=1)\n",
    "#predictions = parallel_model.predict(test_set_data,batch_size=batch_size,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    roc_auc = roc_auc_score(test_set_labels, predictions)\n",
    "    print(\"Test roc auc result: {} \".format(roc_auc))\n",
    "except Error as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(test_set_labels.shape[1]):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_set_labels[:, i], predictions[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    \n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_set_labels.ravel(), predictions.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "lw = 3\n",
    "label = 25\n",
    "plt.plot(fpr[label], tpr[label], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[label])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = predictions[0]\n",
    "print(\"All the same: {}\".format(all([all(i) for i in [test-i<np.finfo(np.float32).eps for i in predictions]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=500\n",
    "print(predictions[a])\n",
    "print(test_set_labels[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "weights = np.concatenate([ i.flatten() for i in model.get_weights() ])\n",
    "print('Are there NaN weights? {}'.format(any([ math.isnan(i) for i in weights])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musictagger",
   "language": "python",
   "name": "musictagger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
