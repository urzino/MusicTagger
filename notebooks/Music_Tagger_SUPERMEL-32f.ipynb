{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emanuele.dallalonga/miniconda3/envs/musictagger/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, zero_one_loss, auc\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import librosa\n",
    "import pickle as pk\n",
    "\n",
    "from keras.layers import Conv2D, MaxPool2D, Activation, Dense, Input, Flatten, BatchNormalization, Dropout\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import Sequence\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hardware Parameters\n",
    "n_gpus = 4\n",
    "\n",
    "#Mel parameters\n",
    "sr = 22050\n",
    "n_sample_fft = 2048 \n",
    "hop_length = 512\n",
    "\n",
    "#Training Parameters\n",
    "batch_size = 32\n",
    "max_epochs = 200\n",
    "max_trainings = 5\n",
    "kernel_initializer = 'glorot_uniform'#'he_uniform'\n",
    "\n",
    "if batch_size % n_gpus != 0:\n",
    "    print(\"Batch size should be dividibile per n_gpus\")\n",
    "\n",
    "# SGD parameters\n",
    "starting_learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "global_decay = 0.2\n",
    "local_decay = 1e-6\n",
    "\n",
    "# EarlyStopping Parameters\n",
    "min_improvement = 0\n",
    "patience = 10\n",
    "\n",
    "# Paths\n",
    "dataset_dir = '../data/MagnaTagATune/MEL_LSTM_V2/'\n",
    "annotations_path = '../data/MagnaTagATune/annotation_reduced_50.csv'\n",
    "\n",
    "checkpoint_dir = './checkpoints_MEL_32f_V3/'\n",
    "checkpoint_file_name = 'weights-{epoch:03d}-{val_loss:.5f}.hdf5'\n",
    "log_dir ='./logs_MEL_32f_V3/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data reading during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MagnaTagATuneSequence(Sequence):\n",
    "\n",
    "    def __init__(self, train_set_paths, train_set_labels, batch_size):\n",
    "        self.paths, self.y = train_set_paths, train_set_labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_paths = self.paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x = []\n",
    "        for value in batch_x_paths:\n",
    "            path = dataset_dir + value[:-3]+'p'\n",
    "            S = pk.load(open(path,'rb'))\n",
    "            batch_x.append(S)\n",
    "        batch_x = np.array(batch_x)[:,:,:,np.newaxis]        \n",
    "        return (batch_x,batch_y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Performance Metrics (not used anymore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_wrong_over_correct_ones(y_true, y_pred):\n",
    "    op1 = K.sum(K.abs(K.cast(y_true - K.round(y_pred), dtype='float32')))\n",
    "    op2 = K.sum(K.cast(K.equal(y_true,1.0),dtype='float32'))\n",
    "    return op1/op2\n",
    "\n",
    "def ratio_correct_ones(y_true, y_pred):\n",
    "    op1 = K.sum(K.cast(K.equal(y_true + K.round(y_pred),2.0),dtype='float32'))\n",
    "    op2 = K.sum(K.cast(K.equal(y_true,1.0),dtype='float32'))\n",
    "    return op1/op2\n",
    "\n",
    "def auc_roc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.metrics.auc(y_true, y_pred, summation_method='careful_interpolation')\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Best checkpoint selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_checkpoint(prev_chkpts):\n",
    "    best_ratio = np.inf\n",
    "    best_chkpt = ''\n",
    "    best_epoch = 0\n",
    "    for chkpt in prev_chkpts:\n",
    "        epoch = int(chkpt[8:11])\n",
    "        ratio = float(chkpt[12:19])\n",
    "        \n",
    "        if ratio < best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_chkpt = chkpt\n",
    "            best_epoch = epoch\n",
    "    print('\\n starting from model {} \\n'.format(best_chkpt))\n",
    "    return best_chkpt, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 292/1920 [00:00<00:00, 2906.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Train set size: 22400\n",
      "Test set size: 3460 \n",
      "\n",
      "Train set size: 20480\n",
      "Validation set size: 1920 \n",
      "\n",
      "X dimension: (128, 261)\n",
      "Y dimension: 50 \n",
      "\n",
      "\n",
      "* * * Loading Validation Set into Memory * * *\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1920/1920 [00:00<00:00, 2882.46it/s]\n"
     ]
    }
   ],
   "source": [
    "annotations = pd.read_csv(annotations_path, sep='\\t')\n",
    "\n",
    "tot_t_size = 0.866203\n",
    "tot_train_set, test_set = train_test_split(annotations, train_size=tot_t_size, test_size=(1-tot_t_size), random_state=42) \n",
    "\n",
    "print(\"Complete Train set size: {}\".format(tot_train_set.shape[0]))\n",
    "print(\"Test set size: {} \\n\".format(test_set.shape[0]))\n",
    "\n",
    "t_size = 0.91429\n",
    "train_set, val_set = train_test_split(tot_train_set, train_size=t_size, test_size=(1-t_size), random_state=42) \n",
    "\n",
    "print(\"Train set size: {}\".format(train_set.shape[0]))\n",
    "print(\"Validation set size: {} \\n\".format(val_set.shape[0]))\n",
    "\n",
    "train_set_paths = train_set['mp3_path'].values\n",
    "train_set_labels = train_set.drop(columns=['mp3_path','Unnamed: 0']).values\n",
    "\n",
    "y_dimension = train_set_labels.shape[1]\n",
    "S = pk.load(open(dataset_dir + annotations['mp3_path'][0][:-3]+ 'p','rb'))\n",
    "x_dimension = S.shape\n",
    "\n",
    "print(\"X dimension: {}\\nY dimension: {} \\n\".format(x_dimension, y_dimension))\n",
    "\n",
    "   \n",
    "val_set_paths = val_set['mp3_path'].values\n",
    "val_set_labels = val_set.drop(columns=['mp3_path','Unnamed: 0']).values\n",
    "    \n",
    "\n",
    "    \n",
    "print('\\n* * * Loading Validation Set into Memory * * *\\n')\n",
    "\n",
    "val_set_data = []\n",
    "for value in tqdm(val_set_paths):\n",
    "    path = dataset_dir+value[:-3]+'p'\n",
    "    S = pk.load(open(path,'rb'))\n",
    "    val_set_data.append(S)  \n",
    "val_set_data = np.array(val_set_data)[:,:,:,np.newaxis] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot train set shape(df): (20480, 52)\n",
      "Tot train set paths: (20480,)\n",
      "Song from train set (df): 4/norine_braun-modern_anguish-09-mr__y-30-59.mp3\n",
      "Song from train set paths: 4/norine_braun-modern_anguish-09-mr__y-30-59.mp3\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "#pick up random song in training\n",
    "#np.random.seed(0)\n",
    "random_song = np.random.randint(0,train_set.shape[0],)\n",
    "song_path = train_set.iloc[random_song]['mp3_path']\n",
    "print('Tot train set shape(df): {}'.format(train_set.shape))\n",
    "print('Tot train set paths: {}'.format(train_set_paths.shape))\n",
    "print('Song from train set (df): {}'.format(train_set.iloc[random_song]['mp3_path']))\n",
    "print('Song from train set paths: {}'.format(train_set_paths[random_song]))\n",
    "\n",
    "labels_from_annotation = annotations.loc[annotations['mp3_path'] == song_path]\n",
    "print(labels_from_annotation.values[0][1:-1])\n",
    "\n",
    "print(train_set_labels[random_song])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Modify session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 32\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=n_filters, kernel_size=(3,3),padding='valid',data_format='channels_last', input_shape=(x_dimension[0],x_dimension[1],1)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=n_filters*2, kernel_size=(3,3),padding='valid',data_format='channels_last', input_shape=(x_dimension[0],x_dimension[1],1)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=n_filters*2, kernel_size=(3,3),padding='valid',data_format='channels_last', input_shape=(x_dimension[0],x_dimension[1],1)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=n_filters*2, kernel_size=(3,3),padding='valid',data_format='channels_last', input_shape=(x_dimension[0],x_dimension[1],1)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=y_dimension, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 126, 259, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 126, 259, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 126, 259, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 63, 129, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 63, 129, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 61, 127, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 61, 127, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 61, 127, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 61, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 61, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 61, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 14, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 14, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 14, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5376)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                268850    \n",
      "=================================================================\n",
      "Total params: 362,418\n",
      "Trainable params: 361,970\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Callbacks definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallBack(keras.callbacks.Callback):\n",
    "    def __init__(self, callbacks, model, is_tb=False):\n",
    "            super().__init__()\n",
    "            self.callback = callbacks\n",
    "            self.is_tb = is_tb\n",
    "            if not self.is_tb:\n",
    "                self.model = model\n",
    "                self.model_original = model\n",
    "\n",
    "    def on_epoch_begin(self,epoch,logs=None):\n",
    "            if not self.is_tb:\n",
    "                self.model = self.model_original\n",
    "            self.callback.on_epoch_begin(epoch, logs=logs)\n",
    "\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "            if not self.is_tb:\n",
    "                self.model = self.model_original\n",
    "            else:\n",
    "                y_pred = self.model.predict(self.validation_data[0])\n",
    "                auc_skl = roc_auc_score(self.validation_data[1], y_pred)\n",
    "                print('\\nSKLearn validation auc: {}'.format(auc_skl))\n",
    "            self.callback.on_epoch_end(epoch, logs=logs)\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "            if not self.is_tb:\n",
    "                self.model = self.model_original\n",
    "            self.callback.on_batch_end(batch, logs=logs)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "            if not self.is_tb:\n",
    "                self.model = self.model_original\n",
    "            self.callback.on_batch_begin(batch, logs=logs)\n",
    "            \n",
    "    def on_train_begin(self, logs=None):\n",
    "            if not self.is_tb:\n",
    "                self.model = self.model_original\n",
    "            self.callback.set_model(self.model)\n",
    "            self.callback.on_train_begin(logs=logs)\n",
    "\n",
    "\n",
    "cbk_tb = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, batch_size=batch_size, write_graph=True,\n",
    "                                         write_grads=False, write_images=False, embeddings_freq=0,\n",
    "                                         embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "cbk_es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n",
    "                                          min_delta=min_improvement, patience=patience, verbose=1)\n",
    "\n",
    "cbk_mc = keras.callbacks.ModelCheckpoint(monitor='val_loss', mode='min', save_best_only=True, \n",
    "                                            filepath=checkpoint_dir+checkpoint_file_name, \n",
    "                                            verbose=1)\n",
    "\n",
    "cbk = MyCallBack(cbk_tb, model, is_tb=True)\n",
    "cbk2 = MyCallBack(cbk_mc, model)\n",
    "\n",
    "callbacks = [cbk,cbk_es,cbk2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 0\n",
    "training_nr = 0\n",
    "\n",
    "parallel_model = keras.utils.multi_gpu_model(model, gpus=n_gpus)\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "\n",
    "while (initial_epoch <= max_epochs) and (training_nr <= max_trainings):\n",
    "    \n",
    "    best_checkpoint = ''\n",
    "    best_epoch = 0\n",
    "    \n",
    "    previous_checkpoints = os.listdir(checkpoint_dir)\n",
    "    \n",
    "    if previous_checkpoints != []:\n",
    "        best_checkpoint, best_epoch = find_best_checkpoint(previous_checkpoints)\n",
    "        initial_epoch = best_epoch             \n",
    "    \n",
    "    print('\\n\\n* * * * Starting training {0} from epoch {1} * * * * \\n\\n'.format(training_nr,  initial_epoch+1))\n",
    "    \n",
    "    #update lr\n",
    "    decay = global_decay ** training_nr\n",
    "    learning_rate = starting_learning_rate * decay\n",
    "    \n",
    "    \n",
    "    training_nr = training_nr + 1\n",
    "    \n",
    "    optimizer = SGD(lr=learning_rate, momentum=momentum, decay=local_decay , nesterov=True)\n",
    "    \n",
    "    if len(previous_checkpoints)!=0:\n",
    "        model.load_weights(checkpoint_dir + best_checkpoint)\n",
    "        parallel_model = keras.utils.multi_gpu_model(model, gpus=n_gpus)\n",
    "    \n",
    "    \n",
    "    parallel_model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "    \n",
    "    parallel_model.fit_generator(MagnaTagATuneSequence(train_set_paths, train_set_labels, batch_size),\n",
    "                                 validation_data = (val_set_data, val_set_labels),\n",
    "                                 epochs=max_epochs, callbacks = callbacks, initial_epoch = initial_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_paths = test_set['mp3_path'].values\n",
    "test_set_labels = test_set.drop(columns=['mp3_path','Unnamed: 0']).values\n",
    "test_set_size = len(test_set_paths)\n",
    "print(\"Test set size: {} \".format(test_set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_checkpoints = os.listdir(checkpoint_dir)\n",
    "best_checkpoint, best_epoch = find_best_checkpoint(previous_checkpoints)\n",
    "#model.load_weights(checkpoint_dir + best_checkpoint)\n",
    "model = keras.models.load_model(checkpoint_dir + best_checkpoint)\n",
    "parallel_model = keras.utils.multi_gpu_model(model, gpus=n_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = parallel_model.predict_generator(MagnaTagATuneSequence(test_set_paths, test_set_labels, batch_size), verbose=1)\n",
    "#predictions = parallel_model.predict(test_set_data,batch_size=batch_size,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    roc_auc = roc_auc_score(test_set_labels, predictions)\n",
    "    print(\"Test roc auc result: {} \".format(roc_auc))\n",
    "except Error as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(test_set_labels.shape[1]):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_set_labels[:, i], predictions[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    \n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_set_labels.ravel(), predictions.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "lw = 3\n",
    "label = 25\n",
    "plt.plot(fpr[label], tpr[label], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[label])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = predictions[0]\n",
    "print(\"All the same: {}\".format(all([all(i) for i in [test-i<np.finfo(np.float32).eps for i in predictions]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=500\n",
    "print(predictions[a])\n",
    "print(test_set_labels[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "weights = np.concatenate([ i.flatten() for i in model.get_weights() ])\n",
    "print('Are there NaN weights? {}'.format(any([ math.isnan(i) for i in weights])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musictagger",
   "language": "python",
   "name": "musictagger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
