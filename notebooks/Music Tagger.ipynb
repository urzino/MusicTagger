{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, zero_one_loss, auc\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from keras.layers import Conv1D, MaxPool1D, Activation, Dense, Input, Flatten, BatchNormalization, Dropout\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import Sequence\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data reading during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MagnaTagATuneSequence(Sequence):\n",
    "\n",
    "    def __init__(self, train_set_paths, train_set_labels, batch_size):\n",
    "        self.paths, self.y = train_set_paths, train_set_labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_paths = self.paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x = []\n",
    "        for value in batch_x_paths:\n",
    "            path = '../data/MagnaTagATune/rawwav/'+value[:-3]+'wav'\n",
    "            _, data = wavfile.read(path)\n",
    "            batch_x.append(data)\n",
    "        batch_x = np.array(batch_x)[:,:,np.newaxis]\n",
    "        return (batch_x,batch_y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_wrong_over_correct_ones(y_true, y_pred):\n",
    "    op1 = K.sum(K.abs(K.cast(y_true - K.round(y_pred), dtype='float32')))\n",
    "    op2 = K.sum(K.cast(K.equal(y_true,1.0),dtype='float32'))\n",
    "    return op1/op2\n",
    "\n",
    "def ratio_correct_ones(y_true, y_pred):\n",
    "    op1 = K.sum(K.cast(K.equal(y_true + K.round(y_pred),2.0),dtype='float32'))\n",
    "    op2 = K.sum(K.cast(K.equal(y_true,1.0),dtype='float32'))\n",
    "    return op1/op2\n",
    "\n",
    "def auc_roc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.metrics.auc(y_true, y_pred)\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Best checkpoint selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_checkpoint(prev_chkpts):\n",
    "    best_ratio = 0\n",
    "    best_chkpt = ''\n",
    "    best_epoch = 0\n",
    "    for chkpt in prev_chkpts:\n",
    "        epoch = int(chkpt[8:11])\n",
    "        ratio = float(chkpt[12:19])\n",
    "        \n",
    "        if ratio > best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_chkpt = chkpt\n",
    "            best_epoch = epoch\n",
    "    print('\\n starting from model {} \\n'.format(best_chkpt))\n",
    "    return best_chkpt, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare Training and Validation  Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = '../data/MagnaTagATune/annotation_reduced.csv'\n",
    "annotations = pd.read_csv(annotations_path, sep='\\t')\n",
    "train_set, test_set = train_test_split(annotations['mp3_path'], train_size=0.7, test_size=0.3) \n",
    "test_set, val_set = train_test_split(test_set, train_size=0.5, test_size=0.5) \n",
    "#train_set= train_set.loc[train_set.str.len()<70]\n",
    "#test_set= test_set.loc[test_set.str.len()<70]\n",
    "\n",
    "train_set_paths = train_set.values\n",
    "train_set_labels = annotations.loc[annotations['mp3_path'].isin(train_set)].drop(columns=['mp3_path','Unnamed: 0']).values\n",
    "train_set_size = len(train_set_paths)\n",
    "print(\"Train set size: {} \\n\".format(train_set_size))\n",
    "\n",
    "y_dimension = train_set_labels.shape[1]\n",
    "\n",
    "_, data = wavfile.read( '../data/MagnaTagATune/rawwav_2/' + annotations['mp3_path'][0][:-3]+ 'wav')\n",
    "x_dimension = len(data)\n",
    "\n",
    "print(\"X dimension: {}\\nY dimension: {} \\n\".format(x_dimension, y_dimension))\n",
    "\n",
    "val_set_paths = val_set.values\n",
    "val_set_labels = annotations.loc[annotations['mp3_path'].isin(val_set)].drop(columns=['mp3_path','Unnamed: 0']).values\n",
    "val_set_size = len(val_set_paths)\n",
    "print(\"Validation set size: {} \\n\".format(val_set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Modify session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=3, strides=3, padding='valid', input_shape=(x_dimension,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=3, strides=1, padding='same', input_shape=(x_dimension,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool1D(pool_size=3))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same', input_shape=(x_dimension,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool1D(pool_size=3))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same', input_shape=(x_dimension,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool1D(pool_size=3))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same', input_shape=(x_dimension,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool1D(pool_size=3))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same', input_shape=(x_dimension,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool1D(pool_size=3))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same', input_shape=(x_dimension,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool1D(pool_size=3))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same', input_shape=(x_dimension,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool1D(pool_size=3))\n",
    "\n",
    "model.add(Conv1D(filters=512, kernel_size=3, strides=1, padding='same', input_shape=(x_dimension,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool1D(pool_size=3))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=y_dimension, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hardware Parameters\n",
    "n_gpus = 4\n",
    "\n",
    "#Training Parameters\n",
    "batch_size = 20\n",
    "max_epochs = 200\n",
    "max_trainings = 10\n",
    "\n",
    "# SGD parameters\n",
    "starting_learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "starting_decay = 1e-6\n",
    "\n",
    "# EarlyStopping Parameters\n",
    "min_improvement = 0\n",
    "patience = 10\n",
    "\n",
    "# Directories\n",
    "checkpoint_dir = './checkpoints_2/'\n",
    "checkpoint_file_name = 'weights-{epoch:03d}-{val_auc_roc:.5f}.hdf5'\n",
    "log_dir ='./logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Callbacks definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallBack(keras.callbacks.Callback):\n",
    "    def __init__(self, callbacks,model):\n",
    "            super().__init__()\n",
    "            self.callback = callbacks\n",
    "            self.model = model\n",
    "            self.model_original = model\n",
    "\n",
    "    def on_epoch_begin(self,epoch,logs=None):\n",
    "            self.model = self.model_original\n",
    "            self.callback.on_epoch_begin(epoch, logs=logs)\n",
    "\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "            self.model = self.model_original\n",
    "            self.callback.on_epoch_end(epoch, logs=logs)\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "            self.model = self.model_original\n",
    "            self.callback.on_batch_end(batch, logs=logs)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "            self.model = self.model_original\n",
    "            self.callback.on_batch_begin(batch, logs=logs)\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "            self.model = self.model_original\n",
    "            self.callback.set_model(self.model)\n",
    "            self.callback.on_train_begin(logs=logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "            self.model = self.model_original\n",
    "            self.callback.on_train_end(logs=logs)\n",
    "\n",
    "cbk_tb = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, batch_size=batch_size, write_graph=True,\n",
    "                                         write_grads=False, write_images=False, embeddings_freq=0,\n",
    "                                         embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "cbk_es = keras.callbacks.EarlyStopping(monitor='val_auc_roc', mode='max',\n",
    "                                          min_delta=min_improvement, patience=patience, verbose=1)\n",
    "\n",
    "cbk_mc = keras.callbacks.ModelCheckpoint(monitor='val_auc_roc', mode='max', save_best_only=True, \n",
    "                                            filepath=checkpoint_dir+checkpoint_file_name, \n",
    "                                            verbose=1)\n",
    "\n",
    "cbk = MyCallBack(cbk_tb, model)\n",
    "cbk1 = MyCallBack(cbk_es, model)\n",
    "cbk2 = MyCallBack(cbk_mc, model)\n",
    "\n",
    "callbacks = [cbk,cbk1,cbk2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, batch_size=batch_size, write_graph=True,\n",
    "                                         write_grads=False, write_images=False, embeddings_freq=0,\n",
    "                                         embeddings_layer_names=None, embeddings_metadata=None),\n",
    "            \n",
    "            # Stop training if, after \"patience\" epochs, no one of the two val metrics has improved            \n",
    "            keras.callbacks.EarlyStopping(monitor='val_auc_roc', mode='max',\n",
    "                                          min_delta=min_improvement, patience=patience, verbose=1, ),\n",
    "            \n",
    "            #Save model checkpoint if at least one of the two val metrics has improved            \n",
    "            keras.callbacks.ModelCheckpoint(monitor='val_auc_ro', mode='max', save_best_only=True, \n",
    "                                            filepath=checkpoint_dir+checkpoint_file_name, \n",
    "                                            verbose=1)]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 0\n",
    "training_nr = 0\n",
    "\n",
    "decay = starting_decay\n",
    "learning_rate = starting_learning_rate\n",
    "\n",
    "parallel_model = keras.utils.multi_gpu_model(model, gpus=n_gpus)\n",
    "\n",
    "while (initial_epoch <= max_epochs) and (training_nr <= max_trainings):\n",
    "    print('\\n\\n* * * * Starting training {0} from epoch {1} * * * * \\n\\n'.format(training_nr,  initial_epoch+1))\n",
    "    best_checkpoint = ''\n",
    "    best_epoch = 0\n",
    "    \n",
    "    previous_checkpoints = os.listdir(checkpoint_dir)\n",
    "    \n",
    "    if previous_checkpoints != []:\n",
    "        best_checkpoint, best_epoch = find_best_checkpoint(previous_checkpoints)\n",
    "        initial_epoch = best_epoch      \n",
    "       \n",
    "    if training_nr != 0:        \n",
    "        decay = starting_decay ** training_nr\n",
    "        learning_rate = starting_learning_rate - decay         \n",
    "        \n",
    "    training_nr = training_nr + 1\n",
    "    \n",
    "    optimizer = SGD(lr = learning_rate, momentum = momentum, decay = decay , nesterov=True)\n",
    "    parallel_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[ratio_correct_ones,\n",
    "                                                                            ratio_wrong_over_correct_ones,\n",
    "                                                                            auc_roc]) \n",
    "    \n",
    "    if len(previous_checkpoints)!=0:\n",
    "        model.load_weights(checkpoint_dir + best_checkpoint)\n",
    "        parallel_model = keras.utils.multi_gpu_model(model, gpus=n_gpus)\n",
    "    \n",
    "    \n",
    "    parallel_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[ratio_correct_ones,\n",
    "                                                                            ratio_wrong_over_correct_ones,\n",
    "                                                                            auc_roc]) \n",
    "\n",
    "    parallel_model.fit_generator(MagnaTagATuneSequence(train_set_paths, train_set_labels, batch_size),\n",
    "                        validation_data = MagnaTagATuneSequence(val_set_paths, val_set_labels, batch_size),\n",
    "                        epochs=max_epochs, callbacks = callbacks, initial_epoch = initial_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_paths = test_set.values\n",
    "test_set_labels = annotations.loc[annotations['mp3_path'].isin(test_set)].drop(columns=['mp3_path','Unnamed: 0']).values\n",
    "test_set_size = len(test_set_paths)\n",
    "print(\"Test set size: {} \".format(test_set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_checkpoints = os.listdir(checkpoint_dir)\n",
    "best_checkpoint, best_epoch = find_best_checkpoint(previous_checkpoints)\n",
    "parallel_model = keras.utils.multi_gpu_model(model, gpus=n_gpus)\n",
    "model.load_weights(checkpoint_dir + best_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = parallel_model.predict_generator(MagnaTagATuneSequence(test_set_paths, test_set, batch_size), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    roc_auc = roc_auc_score(test_set_labels, predictions)\n",
    "    print(\"Test roc auc result: {} \".format(roc_auc))\n",
    "except ValueError:\n",
    "    print('ERROR ON TEST ROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(test_set_labels.shape[1]):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_set_labels[:, i], predictions[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    \n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_set_labels.ravel(), predictions.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "lw = 3\n",
    "label = 28\n",
    "plt.plot(fpr[label], tpr[label], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[label])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=20\n",
    "print(predictions[a])\n",
    "print(test_set_labels[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musictagger",
   "language": "python",
   "name": "musictagger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
