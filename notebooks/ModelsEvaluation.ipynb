{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "import pickle as pk\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_type = 'lstm' # 'raw' 'mel' 'lstm'\n",
    "batch_size = 2\n",
    "n_splits = 4\n",
    "n_gpus = 1\n",
    "\n",
    "# Paths\n",
    "dataset_dir = '../data/MagnaTagATune/rawwav_2/'\n",
    "checkpoint_dir = './checkpoints_RAW/'\n",
    "\n",
    "if mod_type == 'mel':\n",
    "    dataset_dir = '../data/MagnaTagATune/mel_default_hop/'\n",
    "    checkpoint_dir = './checkpoints_mel_32f_V3/'\n",
    "    \n",
    "if mod_type == 'lstm':\n",
    "        dataset_dir = '../data/MagnaTagATune/MEL_default_hop/'\n",
    "        checkpoint_dir = './checkpoints_MEL_LSTM_V2.save/'\n",
    "\n",
    "annotations_path = '../data/MagnaTagATune/annotation_reduced_50.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MagnaTagATuneSequenceRaw(Sequence):\n",
    "\n",
    "    def __init__(self, train_set_paths, train_set_labels, batch_size):\n",
    "        self.paths, self.y = train_set_paths, train_set_labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_paths = self.paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x = []\n",
    "        for value in batch_x_paths:\n",
    "            path = dataset_dir + value[:-3]+'wav'\n",
    "            _, data = wavfile.read(path)\n",
    "            batch_x.append(data)\n",
    "        batch_x = np.array(batch_x)[:,:,np.newaxis]        \n",
    "        return (batch_x,batch_y) \n",
    "    \n",
    "class MagnaTagATuneSequenceMEL(Sequence):\n",
    "\n",
    "    def __init__(self, train_set_paths, train_set_labels, batch_size):\n",
    "        self.paths, self.y = train_set_paths, train_set_labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_paths = self.paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x = []\n",
    "        for value in batch_x_paths:\n",
    "            path = dataset_dir + value[:-3]+'p'\n",
    "            S = pk.load(open(path,'rb'))\n",
    "            batch_x.append(S)\n",
    "        batch_x = np.array(batch_x)[:,:,:,np.newaxis]        \n",
    "        return (batch_x,batch_y)  \n",
    "    \n",
    "class MagnaTagATuneSequenceLSTM(Sequence):\n",
    "\n",
    "    def __init__(self, train_set_paths, train_set_labels, batch_size, n_splits):\n",
    "        self.paths, self.y = train_set_paths, train_set_labels\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        path = dataset_dir + self.paths[0][:-3]+'p'\n",
    "        S = pk.load(open(path,'rb'))\n",
    "        timestamps = S.shape[1]\n",
    "        self.n_splits = n_splits\n",
    "        self.split_size = int(timestamps/n_splits)\n",
    "        #print(self.split_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_paths = self.paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x = []\n",
    "        for value in batch_x_paths:\n",
    "            path = dataset_dir + value[:-3]+'p'\n",
    "            S = pk.load(open(path,'rb'))\n",
    "            #print(S)\n",
    "            for split in range(1,(self.n_splits+1)):\n",
    "                splitmat = S.T[ (split-1)*self.split_size : split*self.split_size]\n",
    "                batch_x.append(splitmat)\n",
    "        batch_x = np.array(batch_x)[:,:,:]\n",
    "        batch_y = np.repeat(batch_y, self.n_splits, axis=0)\n",
    "        return (batch_x,batch_y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_checkpoint(prev_chkpts):\n",
    "    best_ratio = np.inf\n",
    "    best_chkpt = ''\n",
    "    best_epoch = 0\n",
    "    for chkpt in prev_chkpts:\n",
    "        epoch = int(chkpt[8:11])\n",
    "        ratio = float(chkpt[12:19])\n",
    "        \n",
    "        if ratio < best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_chkpt = chkpt\n",
    "            best_epoch = epoch\n",
    "    print('\\n starting from model {} \\n'.format(best_chkpt))\n",
    "    return best_chkpt, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_opt_th(y_true, y_pred, th):\n",
    "    y_pred_th = y_pred >= th\n",
    "    return accuracy_score(y_true, y_pred_th)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['0/american_bach_soloists-j_s__bach__cantatas_volume_v-02-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_ii_recitative__gleichwie_der_regen_und_schnee-30-59.mp3',\n",
    "          '0/american_bach_soloists-j_s__bach__cantatas_volume_v-03-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_iii_recitative_and_litany__mein_gott_hier_wird_mein_herze_sein-30-59.mp3',\n",
    "          '0/american_bach_soloists-j_s__bach__cantatas_volume_v-03-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_iii_recitative_and_litany__mein_gott_hier_wird_mein_herze_sein-59-88.mp3',\n",
    "          '0/american_bach_soloists-j_s__bach__cantatas_volume_v-03-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_iii_recitative_and_litany__mein_gott_hier_wird_mein_herze_sein-88-117.mp3',\n",
    "          '0/american_bach_soloists-j_s__bach__cantatas_volume_v-03-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_iii_recitative_and_litany__mein_gott_hier_wird_mein_herze_sein-146-175.mp3',\n",
    "          '0/american_bach_soloists-j_s__bach__cantatas_volume_v-03-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_iii_recitative_and_litany__mein_gott_hier_wird_mein_herze_sein-175-204.mp3',\n",
    "          '0/american_bach_soloists-j_s__bach__cantatas_volume_v-03-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_iii_recitative_and_litany__mein_gott_hier_wird_mein_herze_sein-204-233.mp3',\n",
    "          '0/american_bach_soloists-j_s__bach__cantatas_volume_v-03-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_iii_recitative_and_litany__mein_gott_hier_wird_mein_herze_sein-233-262.mp3',\n",
    "          '0/american_bach_soloists-j_s__bach__cantatas_volume_v-03-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_iii_recitative_and_litany__mein_gott_hier_wird_mein_herze_sein-262-291.mp3',\n",
    "          '0/american_bach_soloists-j_s__bach__cantatas_volume_v-03-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_iii_recitative_and_litany__mein_gott_hier_wird_mein_herze_sein-291-320.mp3',\n",
    "          '6/norine_braun-now_and_zen-08-gently-117-146.mp3',\n",
    "          '2/ensemble_sreteniye___three_holies_church_choristers-dont_cry_rachael-09-who_is_the__great_lord__a_byzantine_tradition_b_bulgarian_tradition_c_russian_tradition-0-29.mp3',\n",
    "          '2/ensemble_sreteniye___three_holies_church_choristers-dont_cry_rachael-09-who_is_the__great_lord__a_byzantine_tradition_b_bulgarian_tradition_c_russian_tradition-0-29.mp3',\n",
    "          '2/ensemble_sreteniye___three_holies_church_choristers-dont_cry_rachael-09-who_is_the__great_lord__a_byzantine_tradition_b_bulgarian_tradition_c_russian_tradition-117-146.mp3',\n",
    "          '2/ensemble_sreteniye___three_holies_church_choristers-dont_cry_rachael-09-who_is_the__great_lord__a_byzantine_tradition_b_bulgarian_tradition_c_russian_tradition-175-204.mp3',\n",
    "          '2/ensemble_sreteniye___three_holies_church_choristers-dont_cry_rachael-09-who_is_the__great_lord__a_byzantine_tradition_b_bulgarian_tradition_c_russian_tradition-204-233.mp3',\n",
    "          '2/ensemble_sreteniye___three_holies_church_choristers-dont_cry_rachael-09-who_is_the__great_lord__a_byzantine_tradition_b_bulgarian_tradition_c_russian_tradition-233-262.mp3',\n",
    "          '2/ensemble_sreteniye___three_holies_church_choristers-dont_cry_rachael-09-who_is_the__great_lord__a_byzantine_tradition_b_bulgarian_tradition_c_russian_tradition-262-291.mp3',\n",
    "          '8/jacob_heringman-josquin_des_prez_lute_settings-19-gintzler__pater_noster-204-233.mp3',\n",
    "          '9/american_baroque-dances_and_suites_of_rameau_and_couperin-25-le_petit_rien_xiveme_ordre_couperin-88-117.mp3']\n",
    "urzi_pc=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(annotations_path, sep='\\t')\n",
    "\n",
    "tot_t_size = 0.866203\n",
    "tot_train_set, test_set = train_test_split(annotations, train_size=tot_t_size, test_size=(1-tot_t_size), random_state=42)\n",
    "\n",
    "t_size = 0.91429\n",
    "train_set, val_set = train_test_split(tot_train_set, train_size=t_size, test_size=(1-t_size), random_state=42)\n",
    "\n",
    "if urzi_pc:\n",
    "    annotations = annotations.drop(index = annotations.loc[annotations['mp3_path'].isin(to_drop)].index)\\\n",
    "                                                                                            .reset_index(drop=True)\n",
    "    train_set = annotations.drop(index = train_set.loc[train_set['mp3_path'].isin(to_drop)].index)\\\n",
    "                                                                                            .reset_index(drop=True)\n",
    "    val_set = val_set.drop(index = val_set.loc[val_set['mp3_path'].isin(to_drop)].index)\\\n",
    "                                                                                            .reset_index(drop=True)\n",
    "    test_set = test_set.drop(index = test_set.loc[test_set['mp3_path'].isin(to_drop)].index)\\\n",
    "                                                                                            .reset_index(drop=True)\n",
    "\n",
    "data_set_paths = annotations['mp3_path'].values\n",
    "data_set_labels = annotations.drop(columns=['mp3_path','Unnamed: 0']).values\n",
    "\n",
    "train_set_paths = train_set['mp3_path'].values\n",
    "train_set_labels = train_set.drop(columns=['mp3_path','Unnamed: 0']).values\n",
    "\n",
    "val_set_paths = val_set['mp3_path'].values\n",
    "val_set_labels = val_set.drop(columns=['mp3_path','Unnamed: 0']).values\n",
    "\n",
    "test_set_paths = test_set['mp3_path'].values\n",
    "test_set_labels = test_set.drop(columns=['mp3_path','Unnamed: 0']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_checkpoints = os.listdir(checkpoint_dir)\n",
    "best_checkpoint, best_epoch = find_best_checkpoint(previous_checkpoints)\n",
    "model = keras.models.load_model(checkpoint_dir + best_checkpoint)\n",
    "if n_gpus > 1:\n",
    "    model = keras.utils.multi_gpu_model(model, gpus=n_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dataset Portion Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_paths = test_set_paths\n",
    "evaluation_labels = test_set_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prediction Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mod_type == 'raw':\n",
    "    predictions = predictions = model.predict_generator(MagnaTagATuneSequenceRaw(evaluation_paths, evaluation_labels, batch_size),\n",
    "                                                        verbose=1)\n",
    "if mod_type == 'mel':\n",
    "    predictions = predictions = model.predict_generator(MagnaTagATuneSequenceMEL(evaluation_paths, evaluation_labels, batch_size),\n",
    "                                                        verbose=1)\n",
    "if mod_type == 'lstm':\n",
    "    predictions = predictions = model.predict_generator(MagnaTagATuneSequenceLSTM(evaluation_paths, evaluation_labels, batch_size, n_splits),\n",
    "                                                        verbose=1)\n",
    "    \n",
    "    predictions_averaged = np.empty((0,50), np.float32)\n",
    "    for i in range(0,predictions.shape[0], n_splits):\n",
    "        beg = i\n",
    "        end = beg + n_splits\n",
    "        predictions_averaged = np.append(predictions_averaged, np.array([np.mean(predictions[beg:end], axis = 0)]), axis = 0)\n",
    "    predictions = predictions_averaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_auc = roc_auc_score(evaluation_labels, predictions)\n",
    "print(\"Global AUC score is : {}\".format(global_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = annotations.columns.values[1:51]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "thresholds = dict()\n",
    "best_th = dict()\n",
    "accuracy = []\n",
    "for i in range(evaluation_labels.shape[1]):\n",
    "    fpr[i], tpr[i], thresholds[i] = roc_curve(evaluation_labels[:, i], predictions[:, i])\n",
    "    best_th[i] = (np.argmin(np.abs(tpr[i] - (1-fpr[i]))) , thresholds[i][np.argmin(np.abs(tpr[i] - (1-fpr[i])))])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    accuracy = np.append(accuracy,accuracy_opt_th(evaluation_labels[:, i], predictions[:, i], best_th[i][1]))\n",
    "    \n",
    "with open('best_th_'+mod_type+'.p', 'wb') as handle:\n",
    "    pk.dump(best_th, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_th = pk.load(open('best_th_'+mod_type+'.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tpr_per_label = []\n",
    "for label in range(evaluation_labels.shape[1]):\n",
    "    best_tpr_per_label = np.append(best_tpr_per_label,tpr[label][best_th[label][0]])\n",
    "    \n",
    "print('TPR : {}'.format(np.mean(best_tpr_per_label)))\n",
    "    \n",
    "best_fpr_per_label = []\n",
    "for label in range(evaluation_labels.shape[1]):\n",
    "    best_fpr_per_label = np.append(best_fpr_per_label,fpr[label][best_th[label][0]])\n",
    "\n",
    "print('FPR : {}'.format(np.mean(best_fpr_per_label)))\n",
    "\n",
    "auc_per_label = []\n",
    "for label in range(evaluation_labels.shape[1]):\n",
    "    auc_per_label = np.append(auc_per_label,roc_auc[label])\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "ind = np.arange(50)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "accs = np.ones(50)*(np.sum(accuracy)/50)\n",
    "\n",
    "rects1 = ax.bar(ind, accuracy, color='b')\n",
    "rects2 = ax.plot(accs, color='r')\n",
    "\n",
    "ax.legend( (rects1[0], rects2[0]), ('Label Accuracy', 'Global Accuracy'))\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_axisbelow(True)\n",
    "ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "width = 0.35\n",
    "\n",
    "rects1 = ax.bar(ind + width,best_tpr_per_label, width, color='g')\n",
    "rects2 = ax.bar(ind + 2*width, best_fpr_per_label, width, color='r')\n",
    "\n",
    "ax.legend( (rects1[0], rects2[0]), ('TP Rate', 'FP Rate'))\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_axisbelow(True)\n",
    "ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "\n",
    "\n",
    "aucs = np.ones(50)*global_auc\n",
    "\n",
    "rects1 = ax.bar(ind, auc_per_label, color='b')\n",
    "rects2 = ax.plot(aucs, color='r')\n",
    "\n",
    "ax.legend( (rects1[0], rects2[0]), ('Label AUC', 'Global AUC'))\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_axisbelow(True)\n",
    "ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict new song from .wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, song = wavfile.read('../data/MagnaTagATune/songs/back.wav')\n",
    "predictions = model.predict(song[np.newaxis,:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_predictions=np.zeros(50)\n",
    "for (key,value) in best_th.items():\n",
    "    th_predictions[key] = predictions[0][key] > value[1]\n",
    "th_predictions_idx = np.where(th_predictions==1)\n",
    "predicted_labels = annotations.columns.values[1:-1][th_predictions_idx]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict new song mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = pk.load(open('../data/MagnaTagATune/MEL_songs/starwars.p','rb'))\n",
    "predictions = model.predict(song[np.newaxis,:,:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_predictions=np.zeros(50)\n",
    "for (key,value) in best_th.items():\n",
    "    th_predictions[key] = predictions[0][key] > value[1]\n",
    "th_predictions_idx = np.where(th_predictions==1)\n",
    "predicted_labels = annotations.columns.values[1:-1][th_predictions_idx]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict new song LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = pk.load(open('../data/MagnaTagATune/MEL_LSTM_songs/starwars.p','rb'))\n",
    "split_song =[]\n",
    "timestamps = song.shape[1]\n",
    "split_size = int(timestamps/n_splits)\n",
    "for split in range(1,n_splits+1):\n",
    "        splitmat = song.T[ (split-1)*split_size : split*split_size]\n",
    "        split_song.append(splitmat)\n",
    "split_song= np.array(split_song)[:,:,:]\n",
    "predictions = model.predict(split_song)\n",
    "predictions = np.mean(predictions, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_predictions=np.zeros(50)\n",
    "for (key,value) in best_th.items():\n",
    "    th_predictions[key] = predictions[key] > value[1]\n",
    "th_predictions_idx = np.where(th_predictions==1)\n",
    "predicted_labels = annotations.columns.values[1:-1][th_predictions_idx]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = pk.load(open('../data/MagnaTagATune/MEL_LSTM_songs/astronomia.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display as disp\n",
    "%matplotlib qt\n",
    "disp.specshow(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
